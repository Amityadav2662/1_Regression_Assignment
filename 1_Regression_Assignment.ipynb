{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917620e-3bed-450f-85da-7650f1be8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "Ans.\n",
    "1. Simple linear regression:\n",
    "It models the relationship between one independent variable (X) and one dependent variable (Y). The\n",
    "relationship is represented by a straight line equation: Y = bX + a, where a is the y-intercept and \n",
    "b is the slope. For example:Imagine you want to predict a student's exam score (Y) based on their study\n",
    "hours (X). You would use simple linear regression to model the relationship between these two variables\n",
    "and estimate how much a student's score is expected to increase for each additional hour of study.\n",
    "\n",
    "2. Multiple linear regression:\n",
    "It models the relationship between multiple independent variables (X1, X2, ...) and one dependent variable (Y).\n",
    "The relationship becomes more complex, involving multiple coefficients for each independent variable:\n",
    "Y = b1X1 + b2X2 + ... + a, where a is still the y-intercept and b1, b2, ... are the slopes for each \n",
    "independent variable. for example: Predicting house prices based on square footage, number of bedrooms,\n",
    "and neighborhood characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6d300-9e98-4130-bfe9-5b7366ab8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "Ans.\n",
    "Assumptions of Linear Regression:\n",
    "1.Linearity: The relationship between the independent and dependent variables is assumed to be linear.\n",
    "2.Independence: The residuals (errors) are assumed to be independent, meaning there is no systematic pattern\n",
    "in the residuals.\n",
    "3.Homoscedasticity: The variance of the residuals is assumed to be constant across all levels of the independent\n",
    "variable(s).\n",
    "4.Normality: The residuals are assumed to be normally distributed.\n",
    "5.No or little multicollinearity: In multiple linear regression, the independent variables should not be highly\n",
    "correlated.\n",
    "\n",
    "Checking Assumptions:\n",
    "1.Linearity: Use scatterplots and examine the linearity visually or perform statistical tests like the Durbin-Watson\n",
    "test.\n",
    "2.Independence: Check for autocorrelation in residuals using the Durbin-Watson test or plot residuals against time to\n",
    "identify patterns.\n",
    "3.Homoscedasticity: Plot residuals against predicted values or independent variables; use statistical tests like the\n",
    "Breusch-Pagan test.\n",
    "4.Normality: Examine a histogram or a Q-Q plot of residuals, or conduct a normality test (e.g., Shapiro-Wilk test).\n",
    "5.Multicollinearity: Calculate variance inflation factors (VIF) for each independent variable; high VIF values indicate\n",
    "multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7242f7c-d14e-488a-8907-9721cc9614a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "Ans.\n",
    "Interpretation of Slope and Intercept:\n",
    "\n",
    "Slope:\n",
    "Represents the change in the dependent variable for a one-unit change in the independent variable, holding other \n",
    "variables constant.\n",
    "\n",
    "Intercept:\n",
    "Represents the predicted value of the dependent variable when all independent variables are zero.\n",
    "\n",
    "Example:\n",
    "In a linear regression predicting house prices(y) based on the size of the house(x)the slope (b1) might be 100, \n",
    "indicating that, on average, for every additional square foot in the size of the house, the predicted house price\n",
    "increases by $100. The intercept(bo) could be $50,000, representing the estimated house price when the size is zero\n",
    "(which may not have a practical interpretation in this context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaff7f3-f801-494d-b6e6-ac6c2b206a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "Ans. \n",
    "Gradient descent:\n",
    "Gradient descent is an optimization algorithm used in machine learning to minimize the cost function by iteratively\n",
    "adjusting model parameters. It calculates the gradient (slope) of the cost function and updates parameters in the \n",
    "direction that reduces the cost, leading to improved model performance.\n",
    "\n",
    "Gradient Descent in Machine Learning:\n",
    "In machine learning, gradient descent is widely used for training models, such as linear regression, logistic \n",
    "regression, and neural networks. It's a fundamental optimization technique for adjusting the model's parameters to \n",
    "improve its predictive performance. By iteratively updating the parameters based on the negative gradient of the \n",
    "cost function, the algorithm searches for the optimal values that result in the best possible fit to the training\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6f002-3a70-4bb6-bb6d-d4539e246dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "Ans.\n",
    "Multiple linear regression involves predicting a dependent variable based on two or more independent variables.\n",
    "The relationship becomes more complex, involving multiple coefficients for each independent variable:\n",
    "Y = b1X1 + b2X2 + ... + a, where a is still the y-intercept and b1, b2, ... are the slopes for each \n",
    "independent variable. Multiple linear regression extends the concept of simple linear regression by \n",
    "incorporating multiple predictors, capturing more complex relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02f1b7-49e5-40c5-a412-00535b7550a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "Ans.\n",
    "Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly\n",
    "correlated. This correlation can make it challenging to isolate the individual effect of each predictor on the \n",
    "dependent variable.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "1. Correlation Matrix: Examine the correlation matrix among independent variables. High correlation coefficients \n",
    "(close to 1 or -1) indicate potential multicollinearity.\n",
    "2. Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much the \n",
    "variance of an estimated regression coefficient increases if the predictors are correlated. High VIF values \n",
    "(usually above 10) suggest multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "1. Remove Highly Correlated Variables: If two variables are highly correlated, consider removing one of them \n",
    "from the model.\n",
    "2. Feature Engineering: Create new variables by combining or transforming existing ones. This can sometimes\n",
    "reduce multicollinearity.\n",
    "3. Collect More Data: Increasing the dataset size can help mitigate multicollinearity.\n",
    "4. Regularization Techniques: Techniques like Ridge Regression or Lasso Regression introduce regularization \n",
    "penalties that can help reduce the impact of multicollinearity.\n",
    "5. Principal Component Analysis (PCA): Transform the original variables into a set of uncorrelated variables\n",
    "(principal components) to address multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64033219-e31a-4651-b869-dd9e24150019",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "Ans.\n",
    "Polynomial Regression:\n",
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable(x)\n",
    "and the dependent variable (y) is modeled as an nth-degree polynomial. It can capture non-linear patterns in the data.\n",
    "\n",
    "Difference from Linear Regression:\n",
    "Linear Regression: Models the relationship between variables as a linear equation (y = bo + b1x + E), assuming a \n",
    "straight-line relationship.\n",
    "Polynomial Regression: Allows for a curved relationship by introducing higher-degree terms(x^2,x^3....) into the \n",
    "equation, capturing more complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e8eb2-bf5a-49c3-b37b-59687c798599",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?\n",
    "Ans.\n",
    "Advantages of Polynomial Regression over Linear Regression:\n",
    "1. Capturing Non-linearity: Polynomial regression can model more complex relationships in the data, capturing non-linear\n",
    "patterns that linear regression cannot handle.\n",
    "2. Increased Flexibility: The ability to introduce higher-degree terms provides greater flexibility in fitting the model\n",
    "to the data.\n",
    "3. Better Representation of Certain Phenomena: In situations where the relationship between variables is known to be \n",
    "curvilinear, polynomial regression may provide a more accurate representation.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "1. Overfitting: Choosing a high degree for the polynomial can lead to overfitting, where the model fits the training data\n",
    "too closely and fails to generalize well to new data.\n",
    "2. Increased Complexity: As the degree of the polynomial increases, the model becomes more complex, making it harder to\n",
    "interpret and potentially leading to multicollinearity.\n",
    "3. Sensitivity to Outliers: Polynomial regression can be sensitive to outliers, influencing the model disproportionately.\n",
    "4. Loss of Interpretability: Higher-degree polynomials result in equations that are more challenging to interpret, making\n",
    "it difficult to understand the relationship between variables.\n",
    "\n",
    "When to Use Polynomial Regression:\n",
    "1. Non-linear Relationships: When the underlying relationship between the dependent and independent variables is known\n",
    "to be non-linear.\n",
    "2. Curved Patterns: In situations where data exhibits a curve or a bend, polynomial regression may be more suitable.\n",
    "3. Exploratory Data Analysis: As an exploratory tool, polynomial regression can help uncover hidden patterns that linear\n",
    "regression might miss.\n",
    "4. Careful Model Selection: It can be useful when applied judiciously, with attention to selecting an appropriate degree\n",
    "to balance the trade-off between fit and simplicity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
